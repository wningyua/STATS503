---
title: "stats503hw3"
author: "Ningyuan Wang"
date: "2/27/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(503)
```

## Question 1
a. 
As lambda goes to infinity, in $\hat{g_1}$, the first derivative of $g$ becomes a straight line; similarly, in $\hat{g_2}$, the second derivative of $g$ becomes a straight line. In that case, $\hat{g_2}$ will have a smaller training error since it has higher degrees and therefore more flexibility.

b.
As lambda goes to infinity, since we do not know the structure of testing data, we cannot tell which model will have smaller testing error.

c. For lambda = 0, $\hat{g_1}$ and $\hat{g_2}$ basically are the same, and there is no constraint on g($x_i$), and we can always get zero on both $\hat{g_!}$ and $\hat{g_2}$. Therefore, this would be a perfect model fit on training data and the training error can be zero, but the model may not smooth (i.e very wiggly), which  potentially caused overfitting problem on training data. For testing data set, the two models will give same testing error rate but we won't know how large or small of the testing error since no information about testing set. 

## Question 2

### a. 

In the linear regression model, all predictors are significant. R squared is 0.7174, which means around 71.74% variation of cubic root of ozone can be caught with the model. Based on the model diagnosis plots, this is a valid linear model for data and no violation of model assumptions. Overall, this is a good fit. The testing error of the model is 0.318.
```{r}
dat = read.table("ozone_data.txt", header = T)
dat$ozne_cubr = dat$ozone^(1/3)
summary(dat$ozne_cubr) # check missing values
# split dataset 
train_id = sample(1:nrow(dat), floor(0.7*nrow(dat)))
train = dat[train_id, ]
test = dat[-train_id,]

lm.fit = lm(ozne_cubr ~ temperature + wind + radiation, data = train )
summary(lm.fit)

# check model assumpiton
par(mfrow=c(1,2))
qqnorm(lm.fit$residuals, ylab = "Residuals")
qqline(lm.fit$residuals) # normality is good

plot(lm.fit$fitted.values, lm.fit$residuals, xlab = "Fitted", ylab = "Residuals")
abline(h=0) # constant variance


# calculate testing error
prd_test = predict(lm.fit, test)
lm_test_err = mean((test$ozne_cubr - prd_test)^2) #0.186826

```

### b. 

Above linear model has significant advantanges in terms of interpretation and inference. However, it also has significant limitations in terms of prediction power. Therefore, we try to fit a generalized additive model (GAM) to relax linear limitation and achieve a non-linear fit. 


We fit a GAM  in order to predict the cubic root of ozone concentration using smoothing splines on predicotrs temprature, radiation and wind speed. We apply packae "mgcv" to help us decide the optimal degree of freedom based on the rule of minimizing cross validation MSE. 
```{r, warning = FALSE, message=F}
library(gam)
library(mgcv)

```
The plots below show the relationships between the response (cubic root of ozone) and each predictor variable. Based on the plot and summary of the model, we think the model is a good fit of training data set since all predictions are fell in the confidence intervals for each predictor. Also, all smoothing splines on predictors are significant in the model. The GAM has a R-squared value with 73.3% which indicated a good fit.

```{r}
# smoothing spline approach
gam2=gam(ozne_cubr~s(temperature)+s(radiation)+s(wind), method = "GCV.Cp", data= train) 
summary(gam2)
par(mfrow=c(1,3))
plot(gam2 ,col="blue") 

```

### c.

Based on two model appproaches (i.e. linear and non-linear models), linear regression and GAM offer similar testing error: 0.185 for linear regression and 0.161 for GAM. Therefore, we conclude two mdoel approaches worked well to the data. 

```{r, echo = FALSE}
library(ggplot2)

# predicted lines of linear regression
par(mfrow=c(2,2))
# temperature
ggplot(data = test, aes(x = ozne_cubr, y = temperature)) + 
  geom_point(color='red') +
  geom_smooth(method = "lm", se = FALSE)
#radiation
ggplot(data = test, aes(x = ozne_cubr, y = radiation)) + 
  geom_point(color='red') +
  geom_smooth(method = "lm", se = FALSE)
# wind
ggplot(data = test, aes(x = ozne_cubr, y = wind)) + 
  geom_point(color='red') +
  geom_smooth(method = "lm", se = FALSE)
# compare testing errors of two models
 #0.186826


gam_test_err = mean((test$ozne_cubr-predict(gam2,test))^2)
 #0.1611087
```
Based on the above plot sets of single predictor vs response for GAM and linear regression models, we conclude that variables temperature and wind speed have relative non-linear relationships to the response variable. However, their non-linearities were not obvious based on the comparison to the plots on linear regression model. Unremarkable non-linearity of predictors in GAM could be the reason that both linear model and GAM worked for data. 